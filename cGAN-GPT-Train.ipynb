{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torchvision.transforms as transforms\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# from PIL import Image\n",
    "# import os\n",
    "\n",
    "# # Hyperparameters\n",
    "# num_epochs = 2\n",
    "# batch_size = 1\n",
    "# learning_rate = 0.0002\n",
    "# image_size = 256\n",
    "\n",
    "# # Data Transform\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((image_size, image_size)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "# ])\n",
    "\n",
    "# # Custom Dataset\n",
    "# class RainDataset(Dataset):\n",
    "#     def __init__(self, rainy_path, derain_path, transform=None):\n",
    "#         self.rainy_images = os.listdir(rainy_path)\n",
    "#         self.derain_images = os.listdir(derain_path)\n",
    "#         self.rainy_path = rainy_path\n",
    "#         self.derain_path = derain_path\n",
    "#         self.transform = transform\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return min(len(self.rainy_images), len(self.derain_images))\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         rainy_img = Image.open(os.path.join(self.rainy_path, self.rainy_images[idx]))\n",
    "#         derain_img = Image.open(os.path.join(self.derain_path, self.derain_images[idx]))\n",
    "        \n",
    "#         if self.transform:\n",
    "#             rainy_img = self.transform(rainy_img)\n",
    "#             derain_img = self.transform(derain_img)\n",
    "#         return rainy_img, derain_img\n",
    "\n",
    "# # Instantiate Dataset and DataLoader\n",
    "# rainy_path = '/Users/matthew/Jupyter/Thesis/DeRain/JRDR/rain_data_train_Heavy/rain/X2'\n",
    "# derain_path = '/Users/matthew/Jupyter/Thesis/DeRain/JRDR/rain_data_train_Heavy/norain'\n",
    "# dataset = RainDataset(rainy_path, derain_path, transform)\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # Define CycleGAN models (Generator and Discriminator)\n",
    "# # Generator, Discriminator, CycleGAN Loss functions and Training Loop omitted for simplicity.\n",
    "# # Refer to a CycleGAN implementation for details on networks and training steps.\n",
    "\n",
    "# # Training Loop (pseudo-code)\n",
    "# for epoch in range(num_epochs):\n",
    "#     for rainy, derain in dataloader:\n",
    "#         # Forward pass through generators and discriminators\n",
    "#         # Calculate cycle consistency, adversarial, and identity losses\n",
    "#         # Backpropagation and weight update\n",
    "#         pass\n",
    "#     print(f\"Epoch [{epoch+1}/{num_epochs}] completed\")\n",
    "\n",
    "# class Generator(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Generator, self).__init__()\n",
    "#         # Define generator layers (e.g., conv layers, residual blocks)\n",
    "#         pass\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Define forward pass\n",
    "#         return x\n",
    "\n",
    "# # Initialize generators and discriminators\n",
    "# generator_rain_to_clear = Generator()\n",
    "# generator_clear_to_rain = Generator()\n",
    "\n",
    "# # Save model checkpoints after training\n",
    "# torch.save(generator_rain_to_clear.state_dict(), \"generator_rain_to_clear.pth\")\n",
    "# torch.save(generator_clear_to_rain.state_dict(), \"generator_clear_to_rain.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/2], Batch [0/200], Loss_G: 25.7280, Loss_D_rain: 0.5443, Loss_D_clear: 0.4726\n",
      "Epoch [0/2], Batch [50/200], Loss_G: 9.5660, Loss_D_rain: 0.3434, Loss_D_clear: 0.2292\n",
      "Epoch [0/2], Batch [100/200], Loss_G: 7.2384, Loss_D_rain: 0.2221, Loss_D_clear: 0.3183\n",
      "Epoch [0/2], Batch [150/200], Loss_G: 12.2940, Loss_D_rain: 0.2300, Loss_D_clear: 0.1803\n",
      "Epoch [1/2], Batch [0/200], Loss_G: 9.1502, Loss_D_rain: 0.3704, Loss_D_clear: 0.2879\n",
      "Epoch [1/2], Batch [50/200], Loss_G: 10.9011, Loss_D_rain: 0.1657, Loss_D_clear: 0.2540\n",
      "Epoch [1/2], Batch [100/200], Loss_G: 6.4483, Loss_D_rain: 0.1857, Loss_D_clear: 0.2956\n",
      "Epoch [1/2], Batch [150/200], Loss_G: 8.9154, Loss_D_rain: 0.1812, Loss_D_clear: 0.3788\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 10\n",
    "batch_size = 1\n",
    "learning_rate = 0.0002\n",
    "image_size = 256\n",
    "lambda_cycle = 10.0\n",
    "lambda_identity = 5.0\n",
    "\n",
    "# Data Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# Instantiate Dataset and DataLoader\n",
    "rainy_path = '/Users/matthew/Jupyter/Thesis/DeRain/JRDR/rain_data_test_Heavy/rain/X2'\n",
    "derain_path = '/Users/matthew/Jupyter/Thesis/DeRain/JRDR/rain_data_test_Heavy/norain'\n",
    "dataset = RainDataset(rainy_path, derain_path, transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Custom Dataset\n",
    "class RainDataset(Dataset):\n",
    "    def __init__(self, rainy_path, derain_path, transform=None):\n",
    "        self.rainy_images = os.listdir(rainy_path)\n",
    "        self.derain_images = os.listdir(derain_path)\n",
    "        self.rainy_path = rainy_path\n",
    "        self.derain_path = derain_path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.rainy_images), len(self.derain_images))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rainy_img = Image.open(os.path.join(self.rainy_path, self.rainy_images[idx]))\n",
    "        derain_img = Image.open(os.path.join(self.derain_path, self.derain_images[idx]))\n",
    "        \n",
    "        if self.transform:\n",
    "            rainy_img = self.transform(rainy_img)\n",
    "            derain_img = self.transform(derain_img)\n",
    "        return rainy_img, derain_img\n",
    "\n",
    "# Residual Block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, num_residual_blocks=9):\n",
    "        super(Generator, self).__init__()\n",
    "        model = [\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=7, stride=1, padding=3),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        in_features = 64\n",
    "        for _ in range(2):\n",
    "            out_features = in_features * 2\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, kernel_size=3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "        for _ in range(num_residual_blocks):\n",
    "            model += [ResidualBlock(in_features)]\n",
    "        for _ in range(2):\n",
    "            out_features = in_features // 2\n",
    "            model += [\n",
    "                nn.ConvTranspose2d(in_features, out_features, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "        model += [nn.Conv2d(64, out_channels, kernel_size=7, stride=1, padding=3), nn.Tanh()]\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Discriminator (PatchGAN)\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, stride):\n",
    "            return [\n",
    "                nn.Conv2d(in_filters, out_filters, kernel_size=4, stride=stride, padding=1),\n",
    "                nn.InstanceNorm2d(out_filters),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            ]\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(in_channels, 64, stride=2),\n",
    "            *discriminator_block(64, 128, stride=2),\n",
    "            *discriminator_block(128, 256, stride=2),\n",
    "            *discriminator_block(256, 512, stride=1),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)\n",
    "\n",
    "# Instantiate models\n",
    "generator_rain_to_clear = Generator()\n",
    "generator_clear_to_rain = Generator()\n",
    "discriminator_rain = Discriminator()\n",
    "discriminator_clear = Discriminator()\n",
    "\n",
    "# Losses and optimizers\n",
    "adversarial_loss = nn.MSELoss()\n",
    "cycle_loss = nn.L1Loss()\n",
    "identity_loss = nn.L1Loss()\n",
    "\n",
    "optimizer_G = optim.Adam(\n",
    "    list(generator_rain_to_clear.parameters()) + list(generator_clear_to_rain.parameters()), \n",
    "    lr=learning_rate, betas=(0.5, 0.999)\n",
    ")\n",
    "optimizer_D_rain = optim.Adam(discriminator_rain.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "optimizer_D_clear = optim.Adam(discriminator_clear.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (rainy, derain) in enumerate(dataloader):\n",
    "        valid = torch.ones(rainy.size(0), 1, 30, 30, requires_grad=False)\n",
    "        fake = torch.zeros(rainy.size(0), 1, 30, 30, requires_grad=False)\n",
    "\n",
    "        # Train Generators\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        fake_clear = generator_rain_to_clear(rainy)\n",
    "        loss_GAN_rain_to_clear = adversarial_loss(discriminator_clear(fake_clear), valid)\n",
    "        \n",
    "        fake_rain = generator_clear_to_rain(derain)\n",
    "        loss_GAN_clear_to_rain = adversarial_loss(discriminator_rain(fake_rain), valid)\n",
    "\n",
    "        recov_rainy = generator_clear_to_rain(fake_clear)\n",
    "        loss_cycle_rainy = cycle_loss(recov_rainy, rainy)\n",
    "\n",
    "        recov_derain = generator_rain_to_clear(fake_rain)\n",
    "        loss_cycle_derain = cycle_loss(recov_derain, derain)\n",
    "        \n",
    "        loss_identity_rain = identity_loss(generator_clear_to_rain(rainy), rainy)\n",
    "        loss_identity_clear = identity_loss(generator_rain_to_clear(derain), derain)\n",
    "\n",
    "        loss_G = (\n",
    "            loss_GAN_rain_to_clear + loss_GAN_clear_to_rain + \n",
    "            lambda_cycle * (loss_cycle_rainy + loss_cycle_derain) + \n",
    "            lambda_identity * (loss_identity_rain + loss_identity_clear)\n",
    "        )\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Train Discriminators\n",
    "        optimizer_D_rain.zero_grad()\n",
    "        optimizer_D_clear.zero_grad()\n",
    "\n",
    "        loss_real_rain = adversarial_loss(discriminator_rain(rainy), valid)\n",
    "        loss_fake_rain = adversarial_loss(discriminator_rain(fake_rain.detach()), fake)\n",
    "        loss_D_rain = (loss_real_rain + loss_fake_rain) / 2\n",
    "        loss_D_rain.backward()\n",
    "        optimizer_D_rain.step()\n",
    "\n",
    "        loss_real_clear = adversarial_loss(discriminator_clear(derain), valid)\n",
    "        loss_fake_clear = adversarial_loss(discriminator_clear(fake_clear.detach()), fake)\n",
    "        loss_D_clear = (loss_real_clear + loss_fake_clear) / 2\n",
    "        loss_D_clear.backward()\n",
    "        optimizer_D_clear.step()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}], Batch [{i}/{len(dataloader)}], \"\n",
    "                  f\"Loss_G: {loss_G.item():.4f}, Loss_D_rain: {loss_D_rain.item():.4f}, Loss_D_clear: {loss_D_clear.item():.4f}\")\n",
    "\n",
    "\n",
    "# Save models after training\n",
    "torch.save(generator_rain_to_clear.state_dict(), \"/Users/matthew/Jupyter/Thesis/DeRain/generator_rain_to_clear.pth\")\n",
    "torch.save(generator_clear_to_rain.state_dict(), \"/Users/matthew/Jupyter/Thesis/DeRain/generator_clear_to_rain.pth\")\n",
    "torch.save(discriminator_rain.state_dict(), \"/Users/matthew/Jupyter/Thesis/DeRain/discriminator_rain.pth\")\n",
    "torch.save(discriminator_clear.state_dict(), \"/Users/matthew/Jupyter/Thesis/DeRain/discriminator_clear.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
